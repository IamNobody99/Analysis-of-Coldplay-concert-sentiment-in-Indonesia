{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from wordcloud import WordCloud\n",
    "import csv\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Awalnya (Korban Penipuan Tiket Konser Coldpla...</td>\n",
       "      <td>1.660908e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pasangan suami istri pelaku penipuan tiket kon...</td>\n",
       "      <td>1.660593e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{Thread}\\n\\nPolisi menerima 60 laporan dari ko...</td>\n",
       "      <td>1.660850e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @mazzini_gsp: 'Awalnya (Korban Penipuan Tik...</td>\n",
       "      <td>1.660962e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @mazzini_gsp: 'Awalnya (Korban Penipuan Tik...</td>\n",
       "      <td>1.660962e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text            Id\n",
       "0  'Awalnya (Korban Penipuan Tiket Konser Coldpla...  1.660908e+18\n",
       "1  Pasangan suami istri pelaku penipuan tiket kon...  1.660593e+18\n",
       "2  {Thread}\\n\\nPolisi menerima 60 laporan dari ko...  1.660850e+18\n",
       "3  RT @mazzini_gsp: 'Awalnya (Korban Penipuan Tik...  1.660962e+18\n",
       "4  RT @mazzini_gsp: 'Awalnya (Korban Penipuan Tik...  1.660962e+18"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tw = pd.read_csv('KonserColdplay.csv',encoding = 'unicode_escape', delimiter=\";\")\n",
    "data = data_tw\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Text', 'Id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # remove hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # remove RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # remove link\n",
    "    text = re.sub(r'[0-9]+', '', text) # remove numbers\n",
    "    text = text.replace('\\n', ' ') # replace new line into space\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations\n",
    "    text = text.strip(' ') # remove characters space from both left and right text\n",
    "    return text\n",
    "\n",
    "def casefoldingText(text): # Converting all the characters in a text into lower case\n",
    "    text = text.lower() \n",
    "    return text\n",
    "\n",
    "def tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens\n",
    "    text = word_tokenize(text) \n",
    "    return text\n",
    "\n",
    "def filteringText(text): # Remove stopwors in a text\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered \n",
    "    return text\n",
    "\n",
    "def stemmingText(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def toSentence(list_words): # Convert list of words into sentence\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = \"\"\n",
    "data['text_clean'] = data['Text'].apply(cleaningText)\n",
    "data['text_clean'] = data['text_clean'].apply(casefoldingText)\n",
    "data.drop(['Text'], axis = 1, inplace = True)\n",
    "\n",
    "data['text_preprocessed'] = data['text_clean'].apply(tokenizingText)\n",
    "data['text_preprocessed'] = data['text_preprocessed'].apply(filteringText)\n",
    "data['text_preprocessed'] = data['text_preprocessed'].apply(stemmingText)\n",
    "\n",
    "# Membuang duplikat tweets/spam tweets\n",
    "data.drop_duplicates(subset = 'text_clean', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.660908e+18</td>\n",
       "      <td>awalnya korban penipuan tiket konser coldplay ...</td>\n",
       "      <td>[korban, tipu, tiket, konser, coldplay, orang,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.660593e+18</td>\n",
       "      <td>pasangan suami istri pelaku penipuan tiket kon...</td>\n",
       "      <td>[pasang, suami, istri, laku, tipu, tiket, kons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.660850e+18</td>\n",
       "      <td>thread  polisi menerima  laporan dari korban p...</td>\n",
       "      <td>[thread, polisi, terima, lapor, korban, tipu, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.660962e+18</td>\n",
       "      <td>gsp awalnya korban penipuan tiket konser coldp...</td>\n",
       "      <td>[gsp, korban, tipu, tiket, konser, coldplay, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.660962e+18</td>\n",
       "      <td>subdit siber dit reskrimum polda metro jaya me...</td>\n",
       "      <td>[subdit, siber, dit, reskrimum, polda, metro, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id                                         text_clean  \\\n",
       "0  1.660908e+18  awalnya korban penipuan tiket konser coldplay ...   \n",
       "1  1.660593e+18  pasangan suami istri pelaku penipuan tiket kon...   \n",
       "2  1.660850e+18  thread  polisi menerima  laporan dari korban p...   \n",
       "3  1.660962e+18  gsp awalnya korban penipuan tiket konser coldp...   \n",
       "5  1.660962e+18  subdit siber dit reskrimum polda metro jaya me...   \n",
       "\n",
       "                                   text_preprocessed  \n",
       "0  [korban, tipu, tiket, konser, coldplay, orang,...  \n",
       "1  [pasang, suami, istri, laku, tipu, tiket, kons...  \n",
       "2  [thread, polisi, terima, lapor, korban, tipu, ...  \n",
       "3  [gsp, korban, tipu, tiket, konser, coldplay, o...  \n",
       "5  [subdit, siber, dit, reskrimum, polda, metro, ...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Coldplay_Clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
